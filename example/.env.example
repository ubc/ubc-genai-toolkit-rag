# .env file for RAG Example Application

# --- General ---
# Set to true for detailed logging
DEBUG=true

# --- LLM Configuration (using Ollama) ---
# Specifies that we are using the 'ollama' provider
LLM_PROVIDER=ollama
# The endpoint where your local Ollama server is running
LLM_ENDPOINT=http://localhost:11434
# The default model to use for generating answers. Make sure you have pulled this model (e.g., `ollama pull llama3`)
LLM_DEFAULT_MODEL=llama3.1
# No API key is needed for a local Ollama instance
LLM_API_KEY=fakekey

# --- Embeddings Configuration (using local fastembed) ---
# This tells the RAG module to use the built-in fastembed provider, which is simple and runs locally.
EMBEDDINGS_PROVIDER=ubc-genai-toolkit-llm
# EMBEDDINGS_PROVIDER=fastembed
# The model fastembed will use to create vector embeddings. 'BAAI/bge-small-en-v1.5' is a good, lightweight default.
EMBEDDINGS_MODEL=nomic-embed-text
# EMBEDDINGS_MODEL=BAAI/bge-small-en-v1.5

# -----------------------------------------------------------------------------
# --- Chunking Configuration (Optional) ---
# -----------------------------------------------------------------------------
# Uncomment one of the strategies below to override the default simple chunker.
# If no strategy is set, the RAG module will use its internal fallback chunker.

# --- Option 1: Recursive Character ---
# A good general-purpose strategy that splits text based on characters (like "\n\n", "\n", " ")
# and tries to keep related pieces of text next to each other.
CHUNKING_STRATEGY=recursiveCharacter
CHUNKING_SIZE=500
CHUNKING_OVERLAP=50

# --- Option 2: Token-based ---
# This strategy splits text based on language model tokens. It's more precise but
# requires the 'tiktoken' library to be installed if it isn't already.
# CHUNKING_STRATEGY=token
# CHUNKING_SIZE=256
# CHUNKING_OVERLAP=30

# --- Option 3: Simple Paragraph/Sentence ---
# A basic strategy that splits by paragraphs or sentences.
# CHUNKING_STRATEGY=simple
# CHUNKING_SIZE=500
# CHUNKING_OVERLAP=50

# --- Qdrant Vector Store Configuration ---
# The URL for the local Qdrant instance you started with Docker
QDRANT_URL=http://localhost:6333
# A name for the collection where your vectors will be stored
QDRANT_COLLECTION_NAME=rag-example-collection
# The vector size MUST match the embedding model. For 'BAAI/bge-small-en-v1.5', this is 384. 768 for nomic embed text.
QDRANT_VECTOR_SIZE=768
# The distance metric for comparing vectors. 'Cosine' is standard for text embeddings.
QDRANT_DISTANCE_METRIC=Cosine
# No API key is needed for a local, unsecured Qdrant instance
QDRANT_API_KEY=super-secret-dev-key

EMBEDDINGS_API_KEY=abcd
EMBEDDINGS_ENDPOINT=http://localhost:11434